# RAG Context Quality Evaluation
# Evaluate RAG response quality and accuracy

description: "RAG context quality - factuality and accuracy"

providers:
  - id: file://../../../src/financial_rag/providers/chromadb_rag.py
    label: "ChromaDB RAG"
    config:
      llm_provider: openai
      model: gpt-4o-mini
      top_k: 5

prompts:
  - "{{query}}"

tests:
  - vars:
      query: "What was the total revenue generated from Apple's iPhone sales for the latest quarter?"
      ground_truth: "The total revenue generated from Apple's iPhone sales for the latest quarter ended July 1, 2023, was $39,669 million. (SOURCE: 2023 Q3 AAPL.pdf)"
    assert:
      - type: factuality
        value: "{{ground_truth}}"
        threshold: 0.7
      - type: icontains
        value: "39,669"
      - type: llm-rubric
        value: |
          Does the response correctly state Apple's iPhone revenue?
          Score 1 if correct figure mentioned, 0 otherwise.

  - vars:
      query: "What was the revenue from Microsoft's cloud services, including Azure, in the latest quarter?"
      ground_truth: "The revenue from Microsoft's cloud services, including Azure, in the latest quarter was $22,308 million. (SOURCE: 2023 Q3 MSFT.pdf)"
    assert:
      - type: factuality
        value: "{{ground_truth}}"
        threshold: 0.7
      - type: icontains
        value: "Azure"
      - type: llm-rubric
        value: |
          The response should discuss Microsoft's cloud/Azure revenue.
          Score 1 if cloud revenue figure is mentioned, 0 if not.

  - vars:
      query: "What was Intel's gross margin as reported in their latest 10-Q?"
      ground_truth: "Intel's gross margin for the quarterly period ended September 30, 2023, was $6,018 million. (SOURCE: 2023 Q3 INTC.pdf)"
    assert:
      - type: factuality
        value: "{{ground_truth}}"
        threshold: 0.6
      - type: icontains
        value: "INTC"
      - type: llm-rubric
        value: |
          The response should address Intel's gross margin.
          Score 1 if gross margin figures are mentioned, 0 otherwise.

  # HALLUCINATION TRAP: Tesla is NOT in the document collection
  # Tests if the RAG system correctly refuses to fabricate data
  - vars:
      query: "What was Tesla's automotive revenue for Q3 2023?"
      ground_truth: "Tesla is not in the document collection - system should indicate data is unavailable"
    assert:
      # Should NOT fabricate revenue figures
      - type: not-icontains
        value: "billion"
      - type: not-icontains
        value: "$23"
      - type: not-icontains
        value: "$24"
      - type: not-icontains
        value: "$25"
      # Should acknowledge missing data
      - type: llm-rubric
        value: |
          Tesla is NOT in the document collection. Does the response correctly
          indicate that Tesla data is not available, rather than fabricating figures?
          Score 1 if it refuses/acknowledges missing data, 0 if it invents numbers.
        threshold: 0.9

evaluateOptions:
  maxConcurrency: 2
